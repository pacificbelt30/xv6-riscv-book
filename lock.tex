\chapter{ロック}
\label{CH:LOCK}

xv6 を含むほとんどのカーネルは，複数の活動をインタリーブ（訳注: またい
で行ったり来たりすること）します．インタリーブが生じる原因の 1 つはマル
チプロセッサのハードウェアです: xv6 の RISC-V がそうであるように，複数
の CPU は独立に実行を行います．複数の CPU は物理的な RAM を共有してお
り，xv6 はそのことを利用して，全 CPU が読み書きできるデータ構造を管理し
ます．そのような共有が行われることで，ある CPU が更新中のデータを，また
別の CPU が読もうとすることがありえます．あるいは，複数の CPU が同じデー
タを同時に更新しようとすることすらありえます．注意深く設計を行わない限
り，そのような並列アクセスはデータ構造の間違いや破壊を引き起こします．
プロセッサが1つしかない場合であっても，カーネルは大量のスレッドを切り替
えるので，それらの実行がインタリーブされることがあります．その結果，デ
バイスの割り込みハンドラが，割り込まれたコードと同じデータを変更しよう
とすることが，まずいタイミングで起きると，データの破壊を引き起こすこと
があります．\indextext{並行性} (concurrency) という言葉は，マルチプロセッサの並列実
行，スレッドの切り替え，および割込により，複数の命令の流れがインタリー
ブされることを表します．

カーネルには，並行アクセスされるデータが山ほどあります．たとえ
ば，2つのCPU が同時に \texttt{kalloc} を呼び，並行して自由リストの先頭
を取り出そうとすることがありえます．カーネル設計者は，並列処理で性能を
上げたり，応答性を良くするために並行度を上げたいと望みます．その結果と
して，並行処理の正しさを検証するために，たくさんの労力を使うことになり
ます．正しいコードにたどり着く方法はたくさんありますが，中でも理由付け
が簡単なものがあります．並行処理における正しさを目的とした戦略と，それ
を支える抽象化は，\indextext{並行性制御テクニック} (concurrency control
techniques) と呼ばれます．xv6 はシチュエーションに応じて，たくさんの並
行性制御テクニックを利用しますが，さらにたくさんの可能性があります．こ
の章は，広く使われているテクニックである\indextext{ロック}について述べます．

ロックは，相互排除を行うことで，1度に1つの CPU だけがそのロックを取得で
きるように保証します．プログラマがロックと共有データアイテムを関連付け，
コードがそのアイテムを使うときは必ず関連づいたロックを取得することにす
れば，そのアイテムは 1 度に 1 つの CPU のみが使用することになります．これを，
ロックがそのデータアイテムを保護する (protect)，と言います．

この章の残りは，xv6 にロックが必要な理由，xv6 がどうやってロックを実装しているか，
そしてどうやってロックを使うかを説明します．

% The following is most relevant to lock-free code:
% 
% A key observation will be that if you look at some code in
% xv6, you must ask yourself if concurrent code could change
% the intended behavior of the code by modifying data (or hardware resources)
% it depends on.
% You must keep in mind that the compiler may turn a
% single C statement into several machine instructions,
% and that those instructions may execute in a way that is
% interleaved with instructions executing on other CPUs.
% That is, you cannot assume that lines of C code
% on the page are executed atomically.
% Concurrency makes reasoning about correctness difficult.

%% 
\section{競合状態}
%% 

なぜロックが必要か説明するための例として，マルチプロセッサにおいて，ど
の CPU からもアクセスできるリンクリストを考えましょう．このリストには，プッ
シュとポップを行うことができ，並行的に呼ばれることもありえます．xv6 の
メモリアロケータは，実際そのように動いています．
\lstinline{kalloc()} \lineref{kernel/kalloc.c:/^kalloc/} は自由なページのリストからページをポップ
し，\lstinline{kfree()} 
\lineref{kernel/kalloc.c:/^kfree/}
はその自由リストに新しいページをプッシュします．


並行するリクエストがなければ，リストの \lstinline{push} 処理を
次のように実装するかもしれません．
\begin{lstlisting}[numbers=left,firstnumber=1]
    struct element {
      int data;
      struct element *next;
    };
    
    struct element *list = 0;
    
    void
    push(int data)
    {
      struct element *l;
   
      l = malloc(sizeof *l);
      l->data = data;
      l->next = list; (*@\label{line:next}@*)
      list = l;  (*@\label{line:list}@*)
   }
\end{lstlisting}

\begin{figure}[t]
\center
\includegraphics[scale=0.5]{fig/race.pdf}
\caption{競合の例}
\label{fig:race}
\end{figure}
%
この実装は，孤立して実行する限りは正しいです．しかし，複数のコピーが並
行実行するとき，このコードは正しくありません．2つの CPU が同時にプッシュ
をするとき，どちらかが \ref{line:list}~行目を実行する前に，両者が \ref{line:next}~行目を実行して
しまうかもしれません (図~\ref{fig:race} を見てください)．すると，両方のリスト要素
の \lstinline{next} が \lstinline{list} の以前の値を指します．
もし \ref{line:list}~行目で \lstinline{list} への書き込みがリスト 2 分生じると，2 度目の
書き込みが 1 度目を上書きしてしまいます．1 度目に代入した要素は
喪失します．

\ref{line:list}~行目における喪失は，\indextext{競合状態}の例です．競合状態とは，あるメモリの場所
が並行的にアクセスされ，その少なくとも一方が書き込みであるときに生じま
す．競合はバグの兆候であり，更新したはずの内容の喪失（両方が書き込みだっ
た場合）か，更新し終わっていないデータ構造の読み込みを起こします．競合
の結末は関連する CPU がアクセスする正確なタイミングや，メモリシステムが
そのメモリアクセスをどのように並べ替えるかに依存するので，競合に由来す
るエラーは再現してデバッグするのが難しいことがあります．たとえ
ば，\lstinline{push} をデバッグするために途中結果を出力しようとすると，
実行のタイミングが変化して，競合が再現しなくなったりします．

競合を避けるための通常の方法はロックを使うことです．ロックは\indextext{相互排除}を
保証して，\lstinline{push} のセンシティブな行を，1度に1つのCPUしか実行でき
ないようにします．そうすることで，上記のシナリオは起きなくなります．
上記のコードの正しくロックするバージョンは，数行を加えたものです（
黄色でハイライトしてあります）. 
\begin{lstlisting}[numbers=left,firstnumber=6]
   struct element *list = 0;
   (*@\hl{struct lock listlock;}@*)
    	
   void
   push(int data)
   {
     struct element *l;
     l = malloc(sizeof *l); (*@\label{line:malloc}@*)
     l->data = data;
   
     (*@\hl{acquire(\&listlock);} @*)
     l->next = list;     (*@\label{line:next1}@*)
     list = l;           (*@\label{line:list1}@*)
     (*@\hl{release(\&listlock)}; @*)
   }
\end{lstlisting}
\lstinline{acquire} と \lstinline{release} で囲まれた領域を
\indextext{クリティカルセクション}と呼びます．また，このロックは 
\lstinline{list} を守っている，という言い方をします．

「あるロックがあるデータを守る」という言葉が実際に意味するのは，そのロッ
クが，そのデータに関わるある不変量を守っているということです．不変量と
は，あるオペレーション間で保たれるデータ構造の性質を表します．通常，あ
るオペレーションの正しさは，オペレーション開始時に不変量が正しいことを
前提にします．そのオペレーションの途中で一時的に不変量が破れることがあ
るかもしれませんが，オペレーション完了時には不変量は再度保たれていなく
てはいけません．たとえば，リンクリストのケースでの不変量は，
「\lstinline{list} はリストの最初の要素を指すこと」そして「各要素
の \lstinline{next} は，その次の要素を指すこと」です．\lstinline{push} の実装
は，一時的に不変量を破ります．\ref{line:next1}~行目において，\lstinline{l} は次の要素を
指しますが，\lstinline{list} はまだ \lstinline{l} を指していません（\ref{line:list1}~行目で
直ります）．先ほど検討した競合状態が生じたのは，不変量が（一時的に）破
れているときに，第二の CPU がリストの不変量に依存するコードを実行した
ためです．ロックを正しく使うことで，1 度に 1 つの CPU だけがクリティカ
ルセクション内でデータ構造を処理することができるようになり，そのデータ
構造の不変量が破れているときに，他のCPU がデータ構造のオペレーションを
行うことはなくなりました．

ロックのことを，次のように考えることもできます：1度に1つだけ実行するよ
うに，並行処理するクリティカルセクションを\indextext{シリアル化} (Serialization) することで，不変量
を保つもの（ただし，そのクリティカルセクションは，孤立して実行するときに
正しいものとします）．あるいは，次のように考えることもできます: 同じロッ
クで守られているクリティカルセクションは，互いにアトミックであり，ある
クリティカルセクションによる完全な変更のみが見える・不完全な変更は見え
ないようになっている．同じロックを同時に取ろうとすることを，複数のプロ
セスが\indextext{コンフリクト} (conflict) している，もしくは，ロックが競合してい
る (\indextext{contension}) と言います．

なお，\lstinline{acquire} を \lstinline{push} の前方に動かしても 正しさは保た
れます．たとえば，\lstinline{acquire} の呼び出しを，13 行目の前に動かして
も大丈夫です．そうした場合，\lstinline{malloc} の呼び出しもシリアル化され
るため，並列性が落ちます．あとの「ロックを使う」セクションで
は，\lstinline{acquire} と \lstinline{release} の呼び出しをどこにいれるべき
かのガイドラインを説明します．

%% 
\section{Code: ロック}
%% 
xv6 には 2 種類のロックがあります．スピンロックとスリープロックです．ま
ずはスピンロックから説明します．xv6 は，\indexcode{spinlock} 構造体でスピ
ンロックを表現します
\lineref{kernel/spinlock.h:/struct.spinlock/}．
この構造体で重要なフィールド
は \lstinline{locked} で，ロックが使用可能ならばゼロ，使用中ならば非ゼロの
値を持ちます．xv6 では，次のようなコードを実行してロックを取得します:
\begin{lstlisting}[numbers=left,firstnumber=21]
   void
   acquire(struct spinlock *lk) // does not work!
   {
     for(;;) {
       if(lk->locked == 0) {  (*@\label{line:test}@*)
         lk->locked = 1;      (*@\label{line:assign}@*)
         break;
       }
     }
   }
\end{lstlisting}
残念ながら，この実装ではマルチプロセッサにおける相互排除は保証できませ
ん．2つの CPU が同時に \ref{line:test}~行目をにたどり着き，\lstinline{lk->locked} がゼ
ロだと気づき，\ref{line:assign}~行目を実行して 2つの CPU とも ロックを取得することがあ
りえます．その時点で，2 つの CPU がロックを取得したので，相互排除の性質
が破れています．私たちがしなくてはいけないのは，\ref{line:test}~行目と \ref{line:assign}~行目を
\indextext{アトミック}な（すなわち分離不可能な）ステップとして実行することです．

ロックは広く使われているので，マルチコアプロセッサは通常，
\ref{line:test} と \ref{line:assign} 行目を
アトミックに実行する命令を提供します．RISC-V では，
\lstinline{amoswap r, a} 
です．\lstinline{amoswap} 命令はメモリアドレス \texttt{a} の値を読み，
レジスタ \texttt{r} の中身をそのアドレスに書き込みます．つま
り，\lstinline{amoswap} 命令は，それらのレジスタとメモリアドレスの中身を取
り替えるのです．RISC-V は以上の流れを自動で行うとともに，特別なハードウェ
アを用いることで，読み書きを行うあいだに他の CPU がそのメモリアドレスを
使わないように防ぎます．

Xv6 の \indexcode{acquire}
\lineref{kernel/spinlock.c:/^acquire/}
は，ポータブルな C ライ
ブラリを使って，\lstinline{__sync_lock_test_and_set} を呼び出します．
この関数は，内部的に \lstinline{amoswap} 命令を使います．返値は，古い（取り替えたあと
の）\lstinline{lk->locked} の値です．\lstinline{acquire} 関数は，上記の取り替え
をループでくるむことで，ロックが取得できるまで再試行（スピン）します．
各くり返しにおいて，\lstinline{lk->locked} に 1 を取り替えて入れるとともに，
取り替える前の値を確認します．もし前の値が 1 だったら，他の CPU がロッ
クを持っているということであり，ま
た，\lstinline{amoswap} が，\lstinline{lk->locked} の値を書き換えなかったとい
うことでもあります．

ロックが取得できたら，\lstinline{acquire} はデバッグのために，ロックを取得
した CPU を記録します．\lstinline{lk->cpu} フィールドはロックで守られてお
り，ロックを取得している状態でしか書き換えてはいけません．

\indexcode{release} 関数
\lineref{kernel/spinlock.c:/^release/}
は，\lstinline{acquire} の逆
のことをします．すなわち，\lstinline{lk->cpu} の値をクリアし，ロックを解放
します．概念上，\lstinline{release} するには \lstinline{lk->locked} にゼロを
書き込めば十分です．しかし，C の標準は，1 つの代入を，複数のストア命令
に翻訳することを許しています．よって， C における代入は，並行処理のコー
ドという意味において，アトミックでない可能性があります．そのかわ
り，\lstinline{release} は C のライブラリ関数であ
る\lstinline{__sync_lock_release} を呼び出します．これは，アトミックな
代入を行うものです．この関数も，内部的にはRISC-V の \lstinline{amoswap} 命
令を使います．

%% 
\section{Code: ロックを使う}
%% 
xv6 は競合状態を避けるために，さまざまな場所でロックを使います．すでに
見た \lstinline{push} とよく似たシンプルな例は\lstinline{kalloc}
\lineref{kernel/kalloc.c:/^kalloc/}
と \lstinline{free}
\lineref{kernel/kalloc.c:/^free/}.
です．これら
の関数がロックを省略したら何が起きるかは，練習問題の1と2を試してみてく
ださい．おそらく，不正な挙動を引き起こすのは難しいのではないかと思いま
す．ロックのエラーや競合が無いかを安定して試験はするのは難しい
のです．xv6 に競合が残されていても不思議ではありません．

ロックを使う難しさは，ロックを何個使うかと，どのデータ・不変量を
そのロックで保護するかを決めることにあります．基本的な原理がいくつかあります．
第一に，ある CPU から書き込まれ，また別の CPU から読み書きされる変数が
あったら，両者が重ならないようにいつでもロックをつかうべきです．第二に，
ロックは不変量を守るのだということを忘れないでください．もしその不変量
が複数のメモリの場所を含むなら，通常，その全てを 1 つのロックで保護して，
不変量が保たれるようにします．

上記のルールは，いつロックが必要かを述べていますが，ロックが不要な場合
については何も言っていません．しかし，効率のためには，ロックしすぎない
ことが重要なのです．ロックは並列度を減らしてしまうからです．もし並列性
が重要で無いならば，スレッドを 1 つだけにして，ロックのことは忘れてしま
せばよいのです．ごく単純なカーネルでは，カーネルに入るためのロックを 1 つ
だけ持ち，カーネルから出るときに解放することで，マルチプロセッサでも同様
のことを行うことができます（ただし，パイプの読み込みや \lstinline{wait} は
問題を起こすかもしれません）．もともとは単一プロセッサ用だったオペレーティ
ングシステムの多くが，このアプローチ（「大きいカーネルロック」と呼ばれるこ
とがあります）でマルチプロセッサに対応しました．しかし，このアプローチ
は並列性を犠牲にしています．1度に1つの CPU しかカーネルを実行できないの
です．もしカーネルが何か重い計算をすることがあるならば，より粒度の細か
いロックをたくさん使うことで，カーネルが同時に複数のCPU で実行できるよ
うにするほうがより効率的です．

粒度の荒いロックの例として，xv6 の \lstinline{kalloc.c} アロケータは 1 つ
の自由リストを 1 つのロックで保護しています．もし異なる CPU にある複数
のプロセスが，同時にページをアロケートしようとしたら，どちらか
は\texttt{acquire} 内でスピンして待たなくてはいけません．スピンは意味あ
る仕事ではないので，パフォーマンスを低下させます．もしロックの競合が CPU
時間の無視できない部分を閉めるならば，アロケータの設計を見直したら性能
が改善するかもしれません．複数の自由リストをもち，それぞれにロックを持
たせることで，真に並列してアロケーションを行えるようにするのです．

粒度の細かいロックの例として，xv6 はファイルごとに異なるロックを使いま
す．そうすることで，異なるファイルを操作する複数のプロセスは，お互いの
ロックを待つことなく仕事を進めることができます．複数のプロセス
が，同じファイルの異なる部分に対して同時に書き込みをしたいのであ
れば，ファイルをロックするスキームの粒度をさらに細かくすることもありえ
ます．ロックの粒度の決定は，究極的には性能を測定するとともに，複雑さ
を考慮して行う必要があります．

続く章では，xv6 のいろいろな部分の説明をしますが，並行処理のために，xv6 が
ロックを使う例について述べることがあります．図~\ref{fig:locktable} に，xv6 の全ての
ロックをあらかじめ見せておきます．


\begin{figure}[t]
\center
{\footnotesize
  \begin{tabular}{ll} 
\textbf{ロック名} & \textbf{保護する対象} \\ \midrule
\texttt{bcache.lock} & ブロックバッファのキャッシュエントリのアロケーション\\
\texttt{cons.lock} & 出力が混ざるのを避けるためのコンソールハードウェアへのアクセス．\\ 
\texttt{ftable.lock} & ファイルテーブルの \texttt{file} 構造体のアロケーション．\\ 
\texttt{icache.lock} & \texttt{inode} のキャッシュエントリのアロケーション．\\ 
\texttt{vdisk\_lock} & ディスクハードウェアおよび DMA ディスクリプタのキューへのアクセス．\\ 
\texttt{kmem.lock} & メモリアロケーション．\\ 
\texttt{log.lock} & トランザクションログのオペレーション．\\
\texttt{pipe} の \texttt{pi->lock} & 各パイプのオペレーション．\\ 
\texttt{pid\_lock} & \texttt{nexd\_pid} のインクリメント．\\
\texttt{proc} \texttt{p->lock}  & プロセス状態の変更．\\
\texttt{tickslock}  & \texttt{ticks} カウンタのオペレーション．\\
\texttt{inode} の　\texttt{ip->lock} & 各 \texttt{inode} とその中身に関するオペレーション．\\
\texttt{buf} の \texttt{b->lock} & 各ブロックバッファのオペレーション\\ 
  \end{tabular}
}
\caption{xv6 のロック}
\label{fig:locktable}
\end{figure}

%% 
\section{デッドロックとロック順序}
%% 
カーネルにおいて，いくつかのロックを同時に取得するコードの実行パスがあ
るとき，全ての実行パスで同じ順序でロックを取得することが重要です．さも
ないと，\indextext{デッドロック}の危険性があります．xv6 において，ロック A と B を
必要する 2 つの実行パスがあって，実行パス 1 は A から B という順序，実
行パス 2 は B から A という順序でロックを取得するとしましょう．また，ス
レッド T1 が実行パス 1 を実行してロック A を取得し，スレッド T2 が実行
パス 2 を実行してロック B を取得したとします．続いて T1 は ロッ
ク B を，T2 はロック A を取得しようとします．2 つのスレッド
のロック取得は，永久にブロックします．なぜなら，どちらのスレッ
ドにとっても他方のスレッドが必要なロックを取得してお
り，ロック取得がリターンするまでそのロックを解放しないからです．
そのようなデッドロックを避けるには，全ての実行パスは，同じ順序でロック
を取得する必要があります．ロックの取得順序をグローバルに統一しなくては
いけないことは，実質的に，ロックは各関数の仕様の一部であるということで
す．関数を使うプログラムは，ロックが指定された順序で呼出されるように，
正しい順序で関数を呼び出さなくてはなりません．

xv6 には，2 つのロックを連鎖的に取得する場所がたくさんあります．プロセスごとのロック
（各 \lstinline{proc} 構造体に入っているロック）はその一例で，それ
は\lstinline{sleep} の動作原理によります(\ref{CH:SCHED}~章をみてください)．たとえ
ば，\lstinline{consoleintr}
\lineref{kernel/console.c:/^consoleintr/}
は打鍵された文字を扱う
割込ルーチンです．もし改行文字が来たら，そのコンソールの入力を待ってい
る全てのプロセスを起床させなくてはいけません．そのため
に，\lstinline{consoleintr} は \lstinline{cons.lock} を取得し
て \indexcode{wakeup} を呼び，待っているプロセスのロックを順に取得しては起
床させます．その結果，デッドロックを避けるためのグローバルなロック順序
として，各プロセスのロックを取得する前に \lstinline{cons.lock} を取得する
というルールがあります．ファイルシステムのコードに，xv6 における最長の
ロックの連鎖があります．
たとえば，ファイルを作成するときは，次のようなたくさんのロックが必要で
す: そのディレクトリ，新しいファイルの inode, そのディスクのブ
ロックバッファ，ディスクドライバの \lstinline{vdisk_lock}, および呼び出し
元プロセスの \lstinline{p->lock}．デッドロックを避けるために，ファイルシス
テムのコードは常に前述の順序でデッドロックを取得します．

デッドロックを回避するためにロック順序を守ることは，驚くほど難しいこと
です．ロックの順序が，プログラムの論理的な構造と矛盾することすらありま
す．コードモジュール M1 からモジュール M2 を呼びたいのですが，モジュー
ル M2 の中で使われるロックを，モジュール M1 の中で使われるロックよりも
先に取得しなくてはいけない，というような場合です．どのロックを取得する
か，事前にはわからない場合もあります．たとえば，次にどのロックを取得す
るか調べるために，まずあるロックを取得しなくてはいけない，ということが
あります．そのようなシチュエーションは，ファイルシステムにおいてパス名
の各部分を順次見ていくときに置きます．あるい
は，\texttt{wait} や \texttt{exit} において，プロセスのテーブルから子プ
ロセスを探すときも同様です．デッドロックの危険性は，粒度の細かいロック
を使ってロックのスキームを組み立てるときの制約になります．ロックがたく
さんあるということは，それだけデッドロックの可能性が高まることを意味す
るからです．デッドロックを避ける必要性は，カーネルの実装における
主要な課題になることすらあります．

\section{ロックと割込ハンドラ}

xv6 のスピンロックには，スレッドと割込ハンドラで共有するデータを保護し
ているものもあります．たとえば，\lstinline{clockintr} タイマ割込ハンドラは，
カーネルスレッドが \lstinline{sys_sleep} \lineref{kernel/sysproc.c:/ticks0.=.ticks/} から読みだ
した \indexcode{ticks} の分だけ，\lstinline{ticks}
\lineref{kernel/sysproc.c:/ticks0.=.ticks/}
を増やすことがあります．\indexcode{tickslock} ロックが，2つのアクセスをシリアル
化しています．

スピンロックと割込が関わることには，潜在的な危険がありま
す．\indexcode{sys_sleep} が \indexcode{tickslock} を取得しているときに，そ
の CPU がタイマ割込に割り込まれたとしま
す．\lstinline{clockintr} は\lstinline{tickslock} を取得しようとして，誰かが
使っていることに気づき，解放されるまで待ちます．このシチュエーションで
は，\lstinline{tickslock} は永久に解放されません．解放できるの
は \lstinline{sys_sleep} だけです
が，\lstinline{sys_sleep} は\lstinline{clockintr} がリターンするまで実行でき
ないからです．よって CPU はデッドロックして，いずれかのロックを必要とす
るコードもまたフリーズします．

そのようなシチュエーションを避けるために，もしあるスピンロックが割込ハ
ンドラで使われるならば，ある割込が有効な状態でそのロックを取得しては
いけません．xv6 はさらに保守的で，CPU がロックを取得するときはいつも，
その CPU の割込を無効にします．他の CPU で割込が発生し，その割込
の \lstinline{acquire} がスピンロックを解放するのを待つこと
はありえますが，それは別の CPU でのことです．

xv6 は，その CPU がスピンロックを全て手放したら，割込を再び有効化します．
入れ子になったクリティカルセクションを取り扱うためには，ロックの個数を
覚えておく必要があります．\lstinline{acquire} は \indexcode{push_off}
\lineref{kernel/spinlock.c:/^push_off/}
を，\lstinline{release} は \indexcode{pop_off}
\lineref{kernel/spinlock.c:/^pop_off/}
をそれぞれ呼び出すことで，その CPU におけるロッ
クの入れ子の深さを調べます．もしその数がゼロになったら，一番外側のクリ
ティカルセクションでの割込の有効状態を \lstinline{pop_off} が復元します．
\lstinline{intr_off} と \lstinline{intr_on} 関数は，RISC-V の命令を
実行することで割込の無効化と有効化をそれぞれ行います．

\indexcode{acquire} による \texttt{push\_off} の呼び出しを，必
ず \texttt{lk->locked} 
\lineref{kernel/spinlock.c:/sync_lock_test_and_set/}
をセットする前に行うこと
は重要です．もし順序が入れ替わってしまうと，割込が有効化されたままロッ
クを所持する瞬間ができてしまいます．その不幸なタイミングに割込が重なる
とシステムがデッドロックします．同様
に，\indexcode{release} が \indexcode{pop_off} を呼ぶのは，ロック
\lineref{kernel/spinlock.c:/sync_lock_release/}
を解放したあとに限ることが重要です．

%% 
\section{命令とメモリの順序}
%% 

ソースコードに書いてある順番でプログラムが実行されると考えるのが自然で
す．しかし，多くのコンパイラや CPU は，性能を上げるためにコードの順序
を変更します．完了に何サイクルもかかる命令があったら，CPU その命令を先
に実行することで，他の命令とオーバーラップして実行させ，CPU がストール
しないようにします．たとえば，CPU は連続する命令 A と B の間に依存性が無
いことに気づき，B の入力が A の入力よりも先に使用可能になったり，命
令 A と B の実行をオーバーラップさせようとすると，命令 B を先に実行する
ことがあります．コンパイラは，同様の並べ替えを行って，ソースコード上では
先行する文の手前に命令列を出力することがあります．

コンパイラと CPU の並べ替えはいくつものルールを守って行われるので，
書かれた順序で実行したときと結果は変わりません．
しかし，そのルールは，並行処理するコードの結果が変わってしまうような
並べ替えは許しており，マルチプロセッサにおいて誤動作を容易に引き起こします
\cite{riscv:user,boehm04}．CPU の並べ替えルールは，\indextext{メモリモデル}と呼ばれます．

たとえば，次の \lstinline{push} のコードにおいて，コンパイラ
か CPU が\ref{line:next2}~行目に対応するストア命令
を，\ref{line:release}~行目にある \lstinline{release} のあとに動かして
しまうと大変なことになります．
%
\begin{lstlisting}[numbers=left,firstnumber=1]
      l = malloc(sizeof *l);
      l->data = data;
      acquire(&listlock);
      l->next = list;   (*@\label{line:next2}@*)
      list = l;      
      release(&listlock);  (*@\label{line:release}@*)
\end{lstlisting}
%
そのような並べ替えが起きると, 別の CPU がロックを取得して更新済
み \lstinline{list} を見れるが，\lstinline{list->next} はまだ初期化され
ていないという瞬間ができてしまいます．

ハードウェアやコンパイラに並べ替えをしてほしくないことを伝えるため
に，xv6 は
\lstinline{acquire}  \lineref{kernel/spinlock.c:/^acquire/} と
\lstinline{release} \lstinline{release} \lineref{kernel/spinlock.c:/^release/} の両方で
\lstinline{__sync_synchronize()} を使っ
ています．\lstinline{__sync_synchronize()} は\indextext{メモリバリア}であり，そのバ
リアを超えてロードやストアを行わないようにコンパイラや CPU に伝えます．
xv6 の \lstinline{acquire} や \lstinline{release} のバリアは，それが問題とな
るほとんどの場合でうまくいきます．xv6 は，共有されたデータの近くでロッ
クを使用するためです．いくつかの例外は \ref{CH:LOCK2}~章で説明します．

%% 
\section{スリープロック}
%% 

% this material refers a lot to sleep, yield, etc, so maybe it should
% be later, after sleep/wakeup in sched.tex.
% maybe in lock2.tex

xv6 はロックを長く持ち続けなくてはいけないことがあります．たとえばファ
イルシステム（\ref{CH:FS}~章）は，ディスクの中身を読み書きするあいだずっと，そのファ
イルをロックし続けます．そのようなディスクへのアクセスには何十ミリ秒もか
かることもあります．長いあいだスピンロックを持ち続けると，別のプロセスが
ロックを取得しようとしていた場合にムダとなります．取得を試みるプロ
セスが長い間スピンしてCPU を浪費するからです．スピンロックの別の欠点は，
スピンロックを持っている間，そのプロセスは CPU を手放すことができないとい
うことです．もし手放すことができれば，そのロックがディスクを待っている間，
別のプロセスが CPU を使うことができます．スピンロックを持っている間
に CPU を手放すことが不正なのは，第二のスレッドがそのスピンロックを取得し
ようとするとデッドロックを起こすからです．\texttt{acquire} は CPU を
手放さないので，第二のスレッドがスピンすると，第一のスレッドがロッ
クを解放できなくなります．ロックを所持しながら CPU を手放すこと
は，スピンロックを所持する間は割込をオフすべしという要求にも違反します．
以上のことを考えると，取得を待つあいだ CPU を手放すことができて，かつロックを
持っているあいだでも CPU を手放せる（また割込を受け付ける）ようなロックが欲
しくなります．

xv6 は，そのようなロックを\indextext{スリープロック}として提供しま
す．\lstinline{acquiresleep} 
\lineref{kernel/sleeplock.c:/^acquiresleep/}
は，\ref{CH:SCHED}~章で述べる方法を
用いて，待っている間 CPU を手放します．おおまかに言うと，スリープロックは
スピンロックで保護されたフィールドを持っており，\lstinline{acquiresleep}
が \lstinline{sleep} を呼ぶと自動で CPU を手放すとともにスピンロックを解放し
ます．その結果，\lstinline{acquiresleep} を待っているあいだ，他のスレッド
が実行を行うことができます．

スリープロックは割込を有効にしたままにするので，割り込みハンドラ内で使
うことができます．\lstinline{acquiresleep} は CPU を手放すかもしれないので，
スリープロックはスピンロックで守られたクリティカルセクションの内部で使
うことができません（その逆に，スリープロックで守られたクリティカルセク
ションの内部でスピンロックを使うことはできます）．

待ち時間が CPU を浪費するので，スピンロックは短いクリティカルセクションには最適です．
それに対し，スリープロックは長く続く処理に向いています．

%% 
\section{世の中のオペレーティングシステム}
%% 
同期プリミティブや並行処理について長い間研究されてきましたが，ロックを
用いたプログラミングは今も難しいままです．xv6 では採用していませんが，より
高級な部品である同期キューなどで内部のロックを隠蔽するのが最適なことが
あります．もしロックを使うプログラミングをするならば，競合状態を検出す
るツールを使うのは賢い選択です．ロックを必要とする不変量は簡単に見逃し
てしまうからです．

ほとんどのオペレーティングシステムは，POSIX スレッド (Pthread) をサポー
トしています．それを使うと，ユーザプロセスを，複数のスレッドを異な
る CPU で並行実行できるようになります．Pthread はユーザレベルのロックや
バリアなどをサポートします．Pthread をサポートするには，オペレーティン
グシステムのサポートが必要です．たとえば，あるスレッドがシステムコール
でブロックしたときでも，同じプロセスの別スレッドは，その CPU で実行でき
なくてはいけません．また別の例として，あるスレッドが（たとえばメモリの
マップやアンマップをして）そのプロセスのアドレス空間を変更したとき，同
じプロセスの別スレッドが，ほかの CPU で実行している可能性があります．そ
の場合，アドレス空間の変更を反映できるように，カーネルが調整を行う必要
があります．

アトミックな命令を用いずにロックを実装することもできますが，それは高コスト
ですので，ほとんどのオペレーティングシステムはアトミックな命令を使いま
す．

たくさんの CPU が 1 つのロックを同時に取得しようとすると，ロックの処理は高
価になります．もしある CPU のローカルキャッシュにロックがキャッシュされ
ており，他の CPU がそのロックを使用しなくてはいけないとき，ロックが入っ
たキャッシュラインを書き換えようとするアトミック命令は，CPU をまたいで
キャッシュのラインを移動させなくてはいけません．それと同時に，そのキャッ
シュラインの別のコピーも期限切れ (invalidate) にするかもしれません．
別の CPU のキャッシュからデータを取得することは，ローカルキャッシュから
取得する場合とくらべて，桁違いに高コストです．

ロックに関連する浪費を避けるために，多くのオペレーティングシステムには
ロック無しデータ構造やアルゴリズムがあります．たとえば，この章の最初に
見せたようなリンクリストを，ロック無しでリストの探索ができ，アトミック命
令1つでリストにアイテムを挿入するように実装することができます．ただし，
ロック無しプログラミングは，ロックを使ったプログラミングよりも複雑です．
ロックを使ったプログラミングがすでにもう難しいので，xv6 はロック無しプ
ログラミングでさらに複雑にすることはしていません．

%% 
\section{練習問題}
%% 

\begin{enumerate}
  
\item \lstinline{kalloc} \lineref{kernel/kalloc.c:/^kalloc/}
において，\lstinline{acquire}
  と \lstinline{release} をコメントアウトしてください．\lstinline{kalloc} を
  呼ぶカーネルのコードで問題が起きそうですが，どんな症状が起きるでしょ
  うか？実際に xv6 を実行したとき，その予想した症状は観察できるでしょう
  か？\lstinline{usertests} を実行しているときではどうでしょうか？もし問題
  が起きないとしたら，それはなぜでしょうか？\lstinline{kalloc} のクリティ
  カルセクションにダミーのループを挿入することで，問題が起こりやすくな
  るか試してみてください．

\item （前の問題の \lstinline{kalloc} のロックを元に戻したあと）かわり
  に \lstinline{kfree} のロックをコメントアウトしたとします．今回はどんな
  まずいことが起きるでしょうか？\lstinline{kfree} のロックがないことは，
  \lstinline{kalloc} のロックがない場合と比べて害が少ないといえるでしょうか？

\item もし 2 つの CPU が同時に \lstinline{kalloc} を呼ぶと，一方はもう一方
  を待たなくてはならないはずで，これはパフォーマンス上よろしくありませ
  ん．\lstinline{kalloc.c} を改造して並列性を上げ，異なる CPU が同時
  に \lstinline{kalloc} を呼んだときでも，お互いを待たずに進めるようにして
  ください．

\item ほとんどのオペレーティングシステムがサポートする POSIX スレッドを
  用いて，並列プログラムを書いてください．たとえば，並列化したハッシュテーブルを
  実装し，puts/gets がコア数の増加に対してスケールするか計測してください．

\item xv6 に，Pthreads のサブセットを実装してください．すなわち，ユーザ
  レベルのスレッドライブラリを実装することで，ユーザプロセスが2 つ以上
  のスレッドを持てるようにし，かつそれらのスレッドが異なる CPU で並列に
  実行できるようにしてください．ブロックするシステムコールを呼んだり，
  仮想アドレス空間を変更するスレッドを扱える設計を考えてください．
\end{enumerate}


% LocalWords:  CPUs uniprocessor interruptible allocator kalloc kfree
% LocalWords:  struct malloc sizeof listlock invariants spinlock lk
% LocalWords:  amoswap cpu bcache ftable icache inode vdisk DMA kmem
% LocalWords:  pid proc's tickslock inode's ip buf's proc SCHED sys
% LocalWords:  consoleintr wakeup clockintr intr acquiresleep POSIX
% LocalWords:  Pthreads pthread unmaps TLB IPIs
